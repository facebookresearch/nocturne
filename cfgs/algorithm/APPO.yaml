algo: APPO
experiments_root: null 
                 # If not None, store experiment data in the specified subfolder of train_dir. Useful for groups of experiments (e.g. gridsearch) (default: None)
train_dir: null
                        # Root for all experiments (default: /private/home/eugenevinitsky/Code/nocturne/examples/train_dir)
                        # if null use the hydra default position
device: gpu  # CPU training is only recommended for smaller e.g. MLP policies (default: gpu)
save_every_sec: 120 # Checkpointing rate (default: 120)
keep_checkpoints: 3 #Number of model checkpoints to keep (default: 3)
save_milestones_sec: -1 #Save intermediate checkpoints in a separate folder for later evaluation (default=never) (default: -1)
stats_avg: 100 #How many episodes to average to measure performance (avg. reward etc) (default: 100)
learning_rate: 0.0001 # LR (default: 0.0001)
train_for_env_steps: 3000000000 # Stop after all policies are trained for this many env steps (default: 10000000000)
train_for_seconds: 10000000000 #Stop training after this many seconds (default: 10000000000)
lr_schedule: constant #Learning rate schedule to use. Constant keeps constant learning rate throughout training.
                  # kl_adaptive* schedulers look at --lr_schedule_kl_threshold and if KL-divergence with behavior policy'
                  # after the last minibatch/epoch significantly deviates from this threshold, lr is apropriately'
                  # increased or decreased
                  # options are 'constant', 'kl_adaptive_minibatch', 'kl_adaptive_epoch'
lr_schedule_kl_threshold: 0.008 #Used with kl_adaptive_* schedulers
obs_subtract_mean: 0.0 # Observation preprocessing, mean value to subtract from observation (e.g. 128.0 for 8-bit RGB) (default: 0.0)
obs_scale: 10.0 # Observation preprocessing, divide observation tensors by this scalar (e.g. 128.0 for 8-bit RGB) (default: 1.0)
gamma: 0.99 # Discount factor (default: 0.99)
reward_scale: 1.0
              # Multiply all rewards by this factor before feeding into RL algorithm.Sometimes the overall scale of rewards is too high which makes value estimation a
              # harder regression task.Loss values become too high which requires a smaller learning rate, etc. (default: 1.0)
reward_clip: 10.0 # Clip rewards between [-c, c]. Default [-10, 10] virtually means no clipping for most envs (default: 10.0)
encoder_type: mlp # Type of the encoder. Supported: conv, mlp, resnet (feel free to define more) (default: conv)
encoder_subtype: mlp_mujoco # Specific encoder design (see model.py) (default: convnet_simple)
encoder_custom: custom_env_encoder # Use custom encoder class from the registry (see model_utils.py) (default: null, options {null, custom_env_encoder})
encoder_extra_fc_layers: 1 # Number of fully-connected layers of size "hidden size" to add after the basic encoder (e.g. convolutional) (default: 1)
encoder_hidden_size: 256
hidden_size: 256 # Size of hidden layer in the model, or the size of RNN hidden state in recurrent model (e.g. GRU) (default: 128)
nonlinearity: tanh #  {elu,relu,tanh}
                  #      Type of nonlinearity to use (default: elu)
policy_initialization: orthogonal #  {orthogonal,xavier_uniform}
                        # NN weight initialization (default: orthogonal)
policy_init_gain: 1.0 # Gain parameter of PyTorch initialization schemas (i.e. Xavier) (default: 1.0)
actor_critic_share_weights: True # Whether to share the weights between policy and value function (default: True)
use_spectral_norm: False # Use spectral normalization to smoothen the gradients and stabilize training. Only supports fully connected layers (default: False)
adaptive_stddev: True # Only for continuous action distributions, whether stddev is state-dependent or just a single learned parameter (default: True)
initial_stddev: 1.0 # Initial value for non-adaptive stddev. Only makes sense for continuous action spaces (default: 1.0)
experiment_summaries_interval: 20 # How often in seconds we write avg. statistics about the experiment (reward, episode length, extra stats...) (default: 20)
adam_eps: 1e-06 # Adam epsilon parameter (1e-8 to 1e-5 seem to reliably work okay, 1e-3 and up does not work) (default: 1e-06)
adam_beta1: 0.9 # Adam momentum decay coefficient (default: 0.9)
adam_beta2: 0.999 # Adam second momentum decay coefficient (default: 0.999)
gae_lambda: 0.95 # Generalized Advantage Estimation discounting (only used when V-trace is False (default: 0.95)
rollout: 20
#    Length of the rollout from each environment in timesteps.Once we collect this many timesteps on actor worker, we send this trajectory to the learner.The
#    length of the rollout will determine how many timesteps are used to calculate bootstrappedMonte-Carlo estimates of discounted rewards, advantages, GAE,
#    or V-trace targets. Shorter rolloutsreduce variance, but the estimates are less precise (bias vs variance tradeoff).For RNN policies, this should be a
#    multiple of --recurrence, so every rollout will be splitinto (n = rollout / recurrence) segments for backpropagation. V-trace algorithm currently
#    requires thatrollout == recurrence, which what you want most of the time anyway.Rollout length is independent from the episode length. Episode length
#    can be both shorter or longer thanrollout, although for PBT training it is currently recommended that rollout << episode_len(see function
#    finalize_trajectory in actor_worker.py) (default: 32)
num_workers: 80 # Number of parallel environment workers. Should be less than num_envs and should divide num_envs (default: 80)
recurrence: 20 # Trajectory length for backpropagation through time. If recurrence=1 there is no backpropagation through time, and experience is shuffled completely
               #         randomlyFor V-trace recurrence should be equal to rollout length. (default: 32)
use_rnn: True #     Whether to use RNN core in a policy or not (default: True)
rnn_type: gru #  {gru,lstm}
              #  Type of RNN cell to use if use_rnn is True (default: gru)
rnn_num_layers: 1 # Number of RNN layers to use if use_rnn is True (default: 1)
ppo_clip_ratio: 0.1 # We use unbiased clip(x, 1+e, 1/(1+e)) instead of clip(x, 1+e, 1-e) in the paper (default: 0.1)
ppo_clip_value: 1.0 # Maximum absolute change in value estimate until it is clipped. Sensitive to value magnitude (default: 1.0)
batch_size: 7180 # Minibatch size for SGD (default: 1024)
num_batches_per_iteration: 1 
# How many minibatches we collect before training on the collected experience. It is generally recommended to set this to 1 for most experiments, because
# any higher value will increase the policy lag.But in some specific circumstances it can be beneficial to have a larger macro-batch in order to shuffle
# and decorrelate the minibatches.Here and throughout the codebase: macro batch is the portion of experience that learner processes per iteration
# (consisting of 1 or several minibatches) (default: 1)
ppo_epochs: 1 # Number of training epochs before a new batch of experience is collected (default: 1)
num_minibatches_to_accumulate: -1
# This parameter governs the maximum number of minibatches the learner can accumulate before further experience collection is stopped.The default value
# (-1) will set this to 2 * num_batches_per_iteration, so if the experience collection is faster than the training,the learner will accumulate enough
# minibatches for 2 iterations of training (but no more). This is a good balance between policy-lag and throughput.When the limit is reached, the learner
# will notify the actor workers that they ought to stop the experience collection until accumulated minibatchesare processed. Set this parameter to 1 *
# num_batches_per_iteration to further reduce policy-lag.If the experience collection is very non-uniform, increasing this parameter can increase overall
# throughput, at the cost of increased policy-lag.A value of 0 is treated specially. This means the experience accumulation is turned off, and all
# experience collection will be halted during training.This is the regime with potentially lowest policy-lag.When this parameter is 0 and num_workers *
# num_envs_per_worker * rollout == num_batches_per_iteration * batch_size, the algorithm is similar toregular synchronous PPO. (default: -1)
max_grad_norm: 4.0 # Max L2 norm of the gradient vector (default: 4.0)
exploration_loss_coeff: 0.001 # Coefficient for the exploration component of the loss function. (default: 0.001)
value_loss_coeff: 0.5 # Coefficient for the critic loss (default: 0.5)
kl_loss_coeff: 0.0 #Coefficient for fixed KL loss (as used by Schulman et al. in https://arxiv.org/pdf/1707.06347.pdf). Highly recommended for environments with continuous
                   #     action spaces. (default: 0.0)
exploration_loss: entropy 
                        # {entropy,symmetric_kl}
                        # Usually the exploration loss is based on maximizing the entropy of the probability distribution. Note that mathematically maximizing entropy of the
                        # categorical probability distribution is exactly the same as minimizing the (regular) KL-divergence between this distribution and a uniform prior. The
                        # downside of using the entropy term (or regular asymmetric KL-divergence) is the fact that penalty does not increase as probabilities of some actions
                        # approach zero. I.e. numerically, there is almost no difference between an action distribution with a probability epsilon > 0 for some action and an
                        # action distribution with a probability = zero for this action. For many tasks the first (epsilon) distribution is preferrable because we keep some
                        # (albeit small) amount of exploration, while the second distribution will never explore this action ever again.Unlike the entropy term, symmetric KL
                        # divergence between the action distribution and a uniform prior approaches infinity when entropy of the distribution approaches zero, so it can prevent
                        # the pathological situations where the agent stops exploring. Empirically, symmetric KL-divergence yielded slightly better results on some problems.
                        # (default: entropy)
max_entropy_coeff: 0.0, # Coefficient for max entropy term added directly to rewards. 0 means no max entropy term to env rewards. '
                        # Note that this is different from exploration loss (see https://arxiv.org/abs/1805.00909)'
num_envs_per_worker: 2
                        # Number of envs on a single CPU actor, in high-throughput configurations this should be in 10-30 range for Atari/VizDoomMust be even for double-buffered
                        # sampling! (default: 2)
worker_num_splits: 2
                        # Typically we split a vector of envs into two parts for "double buffered" experience collectionSet this to 1 to disable double buffering. Set this to 3
                        # for triple buffering! (default: 2)
num_policies: 1
                        # Number of policies to train jointly (default: 1)
policy_workers_per_policy: 1
                        # Number of policy workers that compute forward pass (per policy) (default: 1)
max_policy_lag: 10000
                        # Max policy lag in policy versions. Discard all experience that is older than this. This should be increased for configurations with multiple epochs of
                        # SGD because naturallypolicy-lag may exceed this value. (default: 10000)
traj_buffers_excess_ratio: 1.3
                        # Increase this value to make sure the system always has enough free trajectory buffers (can be useful when i.e. a lot of inactive agents in multi-agent
                        # envs)Decrease this to 1.0 to save as much RAM as possible. (default: 1.3)
decorrelate_experience_max_seconds: 10
                        # Decorrelating experience serves two benefits. First: this is better for learning because samples from workers come from random moments in the episode,
                        # becoming more "i.i.d".Second, and more important one: this is good for environments with highly non-uniform one-step times, including long and expensive
                        # episode resets. If experience is not decorrelatedthen training batches will come in bursts e.g. after a bunch of environments finished resets and many
                        # iterations on the learner might be required,which will increase the policy-lag of the new experience collected. The performance of the Sample Factory is
                        # best when experience is generated as more-or-lessuniform stream. Try increasing this to 100-200 seconds to smoothen the experience distribution in time
                        # right from the beginning (it will eventually spread out and settle anyway) (default: 10)
decorrelate_envs_on_one_worker: True
                        # In addition to temporal decorrelation of worker processes, also decorrelate envs within one worker processFor environments with a fixed episode length
                        # it can prevent the reset from happening in the same rollout for all envs simultaneously, which makes experience collection more uniform. (default: True)
with_vtrace: True
                        # Enables V-trace off-policy correction. If this is True, then GAE is not used (default: True)
vtrace_rho: 1.0
                        # rho_hat clipping parameter of the V-trace algorithm (importance sampling truncation) (default: 1.0)
vtrace_c: 1.0   
                        # c_hat clipping parameter of the V-trace algorithm. Low values for c_hat can reduce variance of the advantage estimates (similar to GAE lambda < 1)
                        # (default: 1.0)
set_workers_cpu_affinity: True
                        # Whether to assign workers to specific CPU cores or not. The logic is beneficial for most workloads because prevents a lot of context switching.However
                        # for some environments it can be better to disable it, to allow one worker to use all cores some of the time. This can be the case for some DMLab
                        # environments with very expensive episode resetthat can use parallel CPU cores for level generation. (default: True)
force_envs_single_thread: True
                        # Some environments may themselves use parallel libraries such as OpenMP or MKL. Since we parallelize environments on the level of workers, there is no
                        # need to keep this parallel semantic.This flag uses threadpoolctl to force libraries such as OpenMP and MKL to use only a single thread within the
                        # environment.Default value (True) is recommended unless you are running fewer workers than CPU cores. (default: True)
reset_timeout_seconds: 120
                        # Fail worker on initialization if not a single environment was reset in this time (worker probably got stuck) (default: 120)
default_niceness: 0
                        # Niceness of the highest priority process (the learner). Values below zero require elevated privileges. (default: 0)
train_in_background_thread: True
                        # Using background thread for training is faster and allows preparing the next batch while training is in progress.Unfortunately debugging can become very
                        # tricky in this case. So there is an option to use only a single thread on the learner to simplify the debugging. (default: True)
learner_main_loop_num_cores: 1
                       # When batching on the learner is the bottleneck, increasing the number of cores PyTorch uses can improve the performance (default: 1)
actor_worker_gpus: []  
                       # [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]
                       # By default, actor workers only use CPUs. Changes this if e.g. you need GPU-based rendering on the actors (default: [])
with_pbt: False        # Enables population-based training basic features (default: False)
pbt_mix_policies_in_one_env: True
                        # For multi-agent envs, whether we mix different policies in one env. (default: True)
pbt_period_env_steps: 5000000
                        # Periodically replace the worst policies with the best ones and perturb the hyperparameters (default: 5000000)
pbt_start_mutation: 20000000
                        # Allow initial diversification, start PBT after this many env steps (default: 20000000)
pbt_replace_fraction: 0.3
                        # A portion of policies performing worst to be replace by better policies (rounded up) (default: 0.3)
pbt_mutation_rate: 0.15
                        # Probability that a parameter mutates (default: 0.15)
pbt_replace_reward_gap: 0.1
                        # Relative gap in true reward when replacing weights of the policy with a better performing one (default: 0.1)
pbt_replace_reward_gap_absolute: 1e-06
                        # Absolute gap in true reward when replacing weights of the policy with a better performing one (default: 1e-06)
pbt_optimize_batch_size: False
                        # Whether to optimize batch size or not (experimental) (default: False)
pbt_optimize_gamma: False
                        # Whether to optimize gamma, discount factor, or not (experimental) (default: False)
pbt_target_objective: true_reward
                        # Policy stat to optimize with PBT. true_reward (default) is equal to raw env reward if not specified, but can also be any other per-policy stat.For
                        # DMlab-30 use value "dmlab_target_objective" (which is capped human normalized score) (default: true_reward)
pbt_perturb_min: 1.05
                        # When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default:
                        # 1.05)
pbt_perturb_max: 1.5
                        # When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default:
                        # 1.5)
use_cpc: False     # Use CPC|A as an auxiliary loss durning learning (default: False)
cpc_forward_steps: 8
                        # Number of forward prediction steps for CPC (default: 8)
cpc_time_subsample: 6
                        # Number of timesteps to sample from each batch. This should be less than recurrence to decorrelate experience. (default: 6)
cpc_forward_subsample: 2
                        # Number of forward steps to sample for loss computation. This should be less than cpc_forward_steps to decorrelate gradients. (default: 2)
with_wandb: ${wandb}
                        # Enables Weights and Biases integration (default: False)
wandb_user: null
                        # WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None)
wandb_project: ${wandb_project}
                        # WandB "Project" (default: sample_factory)
wandb_group: ${wandb_group}
                        # WandB "Group" (to group your experiments). By default this is the name of the env. (default: None)
wandb_job_type: SF
                        # WandB job type (default: SF)
wandb_tags: []          # [WANDB_TAGS [WANDB_TAGS ...]]
                        # Tags can help with finding experiments in WandB web console (default: [])
benchmark: False
                        # Benchmark mode (default: False)
sampler_only: False
                        # Do not send experience to the learner, measuring sampling throughput (default: False)
env_frameskip: null
                        # Number of frames for action repeat (frame skipping). Default (None) means use default environment value (default: None)
env_framestack: 4
                        # Frame stacking (only used in Atari?) (default: 4)
pixel_format: CHW
                        # PyTorch expects CHW by default, Ray & TensorFlow expect HWC (default: CHW)