algorithm_name: 'rmappo' # choices=["rmappo", "mappo"]
experiment: ${experiment}
seed: ${seed}
device: ${device}
cuda_deterministic: True
n_training_threads: 1 # "Number of torch threads for training"
n_rollout_threads: 1 # Number of parallel envs for training rollouts
n_eval_rollout_threads: 1 # Number of parallel envs for evaluating rollouts
n_render_rollout_threads: 1 # Number of parallel envs for rendering rollouts
num_env_steps: 1e8 # Number of environment steps to train
wandb: ${wandb}
use_obs_instead_of_state: True # Whether to use global state or concatenated obs
episode_length: ${episode_length} # Max length for any episode
share_policy: True # Whether all agents share the same policy
use_centralized_V: False # Whether to use a centralized value function
stacked_frames: 1 # number of stacked observations
use_stacked_frames: True # whether to use stacked frames
hidden_size: 64 # Dimension of hidden layers for actor/critic networks
layer_N: 2 # "Number of layers for actor/critic networks"
use_ReLU: True # Whether to use ReLU activation or Tanh
use_popart: False # Use PopART to normalize rewards
use_valuenorm: True # use running mean and std to normalize rewards
use_feature_normalization: True # Whether to apply layernorm to the inputs
use_orthogonal: True # Whether to use Orthogonal initialization for weights and 0 initialization for biases
gain: 0.01 # The gain # of last action layer
# recurrent parameters
use_naive_recurrent_policy: False # Whether to use a naive recurrent policy by stacking states I believe?
use_recurrent_policy: True # Whether to use a recurrent policy
recurrent_N: 1 # The number of recurrent layers
data_chunk_length: 10 # Time length of chunks used to train a recurrent_policy

# optimizer parameters
lr: 5e-4 # learning rate
critic_lr: 5e-4 # critic LR
opti_eps: 1e-5 # RMSprop optimizer epsilon
weight_decay: 0 

# ppo parameters
ppo_epoch: 10 # number of PPO epochs
use_clipped_value_loss: True # clip loss value
clip_param: 0.2 # PPO clipping parameter
num_mini_batch: 4 # Number of minibatches of the collected data to use
entropy_coef: 0.00
value_loss_coef: 0.5 # scaling on the value loss
use_max_grad_norm: True # use max norm of gradients
max_grad_norm: 10.0 # max norm of gradients
use_gae: True # use generalized advantage estimation
gamma: 0.99 # discount factor
gae_lambda: 0.95
use_proper_time_limits: False # compute returns taking into account time limits
use_huber_loss: True 
use_value_active_masks: True # whether to mask useless data in value loss
use_policy_active_masks: True # whether to mask useless data in policy loss
huber_delta: 10.0 # coefficient of huber loss
use_linear_lr_decay: False

# saving and logging
save_interval: 1 # time duration between contiunous twice models saving
log_interval: 5 # time duration between contiunous twice log printing
use_eval: True
eval_interval: 25 
eval_episodes: 10
save_gifs: True
render_interval: 25 # how often to render
use_render: False
render_episodes: 1
ifi: 0.1 # the play interval of each rendered image in saved video
model_dir: null

# goal env wrapper stuff
density_buffer_size: 100000
density_optim_samples: 1000
num_goal_samples: 200
bandwidth: 0.1
log_figure: True
kernel: 'gaussian' 
quartile_cutoff: 0.0
normalize_value: 400.0
log_every_n_episodes: 50
# if True, all the agents share the same goal buffer for sampling new goals
share_goal_buffer: False