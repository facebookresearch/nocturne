defaults:
  - algorithm: ppo
  - override hydra/launcher: submitit_local

seed: 0
device: 'cuda:0'
debug: False
experiment: intersection
env: my_custom_multi_env_v1 # name of the env, harcoded for now

# WANDB things
wandb: True
wandb_project: nocturne4
wandb_id: null
wandb_group: ${experiment}

# one of the agents will be randomly tagged as the 
# agent that we control, the rest of the agents will
# replay trajectories
single_agent_mode: False
# all goals are achievable within 90 steps
episode_length: 80
# how many files of the total dataset to use. -1 indicates to use all of them
num_files: -1
scenario_path: '/checkpoint/eugenevinitsky/waymo_open/motion_v1p1/uncompressed/scenario/formatted_json_v2_no_tl_train'
dt: 0.1
sims_per_step: 10
img_as_state: False
discretize_actions: True
accel_discretization: 6
accel_lower_bound: -3
accel_upper_bound: 2
steering_lower_bound: -0.7 # corresponds to about 40 degrees of max steering angle
steering_upper_bound: 0.7 # corresponds to about 40 degrees of max steering angle
steering_discretization: 21
head_angle_lower_bound: -1.6
head_angle_upper_bound: 1.6
head_angle_discretization: 5
max_num_vehicles: 20 # we want to upper bound how many agents there can be in the scene
                     # this is mostly useful because RL libraries expect it
# TODO(eugenevinitsky) actually implement this
randomize_goals: False
scenario:
  # initial timestep of the scenario (which ranges from timesteps 0 to 90)
  start_time: 0
  # if set to True, non-vehicle objects (eg. cyclists, pedestrians...) will be spawned
  allow_non_vehicles: False
  # for an object to be included into moving_objects
  moving_threshold: 0.2  # its goal must be at least this distance from its initial position
  speed_threshold: 0.05  # its speed must be superior to this value at some point
  # maximum number of each objects visible in the object state
  # if there are more objects, the closest ones are prioritized
  # if there are less objects, the features vector is padded with zeros
  max_visible_objects: 16
  max_visible_road_points: 1000
  max_visible_traffic_lights: 20
  max_visible_stop_signs: 4
  # from the set of road points that comprise each polyline, we take
  # every n-th one of these
  sample_every_n: 1
  # if true we add all the road-edges (the edges you can collide with)
  # to the visible road points first and only add the other points
  # (road lines, lane lines) etc. if we have remaining states after
  road_edge_first: False

# these configs are mostly used for aligning displacement error computations
# with the standard way of doing it in other libraries i.e. we keep 
# the agent for the whole rollout and compute its distance from the expert
# at all the points that the expert is valid
remove_at_goal: True # if true, remove the agent when it reaches its goal
remove_at_collide: True # if true, remove the agent when it collides

rew_cfg:
  shared_reward: False # agents get the collective reward instead of individual rewards
  goal_tolerance: 0.5
  reward_scaling: 10.0 # rescale all the rewards by this value. This can help w/ some learning algorithms
  collision_penalty: 0
  shaped_goal_distance_scaling: 0.2
  shaped_goal_distance: True
  goal_distance_penalty: False # if shaped_goal_distance is true, then when this is True the goal distance 
                               # is a penalty for being far from 
                               # goal instead of a reward for being close
  goal_achieved_bonus: ${episode_length}
  # goal is only achieved if you're within this tolerance on distance from goal
  position_target: True
  position_target_tolerance: 1.0
  # goal is only achieved if you're within this tolerance on final agent speed at goal position
  speed_target: True
  speed_target_tolerance: 1.0
  # goal is only achieved if you're within this tolerance on final agent heading at goal position
  heading_target: True
  heading_target_tolerance: 0.3
subscriber:
  view_angle: 2.1
  # the distance which the cone extends before agents are not visible
  # TODO(eugenevinitsky) pick the right number
  view_dist: 80
  use_ego_state: True
  use_observations: True
  # if true, we return an observation for agents that have exited the system
  # as well as returning an observation for the extra agents if the number of
  # agents in the system is less than max_num_vehicles
  keep_inactive_agents: False
  # for values greater than 1, we will stack inputs together
  n_frames_stacked: 1

hydra:
  run:
    dir: /checkpoint/eugenevinitsky/nocturne/test/${now:%Y.%m.%d}/${experiment}/${now:%H.%M.%S}/${hydra.job.override_dirname}
  sweep:
    dir: /checkpoint/eugenevinitsky/nocturne/sweep/${now:%Y.%m.%d}/${experiment}/${now:%H.%M.%S}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 2880
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: /checkpoint/eugenevinitsky/nocturne/sweep/${now:%Y.%m.%d}/${now:%H%M}_${experiment}/.slurm
